{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw4_Veronika_Kataeva_J41322c_1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Soft deadline: `30.03.2022 23:59`"],"metadata":{"id":"nXpkXz1QJmhJ"}},{"cell_type":"markdown","metadata":{"id":"yxy7euTpCbGp"},"source":["In this homework you will understand the fine-tuning procedure and get acquainted with Huggingface Datasets library"]},{"cell_type":"code","metadata":{"id":"_1MXcMymXeCx"},"source":["! pip install datasets\n","! pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FykUK-TFXf-2"},"source":["For our goals we will use [Datasets](https://huggingface.co/docs/datasets/) library and take `yahoo_answers_topics` dataset - the task of this dataset is to divide documents on 10 topic categories. More detiled information can be found on the dataset [page](https://huggingface.co/datasets/viewer/).\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"SgxNpkGtsL4n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4U4YUOB5W8uG"},"source":["# Fine-tuning the model** (20 points)"]},{"cell_type":"code","metadata":{"id":"ZDYIq9l7CYBR","executionInfo":{"status":"ok","timestamp":1648762620517,"user_tz":-120,"elapsed":11148,"user":{"displayName":"Вероника Катаева","userId":"08423665326520322322"}}},"source":["from transformers import (ElectraTokenizer, ElectraForSequenceClassification,\n","                          get_scheduler, pipeline, ElectraForMaskedLM, ElectraModel)\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from datasets import load_metric"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ElZ6k36rb0VG"},"source":["Fine-tuning procedure on the end task consists of adding additional layers on the top of the pre-trained model. The resulting model can be tuned fully (passing gradients through the all model) or partially."]},{"cell_type":"markdown","metadata":{"id":"pNEmksaPb3Uu"},"source":["**Task**: \n","- load tokenizer and model\n","- look at the predictions of the model as-is before any fine-tuning\n","\n","\n","```\n","- Why don't you ask [MASK]?\n","- What is [MASK]\n","- Let's talk about [MASK] physics\n","```\n","\n","- convert `best_answer` to the input tokens (supporting function for dataset is provided below) \n","\n","```\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"best_answer\"], padding=\"max_length\", truncation=True)\n","\n","tokenized_datasets = dataset.map(tokenize_function, batched=True)\n","```\n","\n","- define optimizer, sheduler (optional)\n","- fine-tune the model (write the training loop), plot the loss changes and measure results in terms of weighted F1 score\n","- get the masked word prediction (sample sentences above) on the fine-tuned model, why the results as they are and what should be done in order to change that (write down your answer)\n","- Tune the training hyperparameters (and write down your results).\n","\n","**Tips**:\n","- The easiest way to get predictions is to use transformers `pipeline` function \n","- Do not forget to set `num_labels` parameter, when initializing the model\n","- To convert data to batches use `DataLoader`\n","- Even the `small` version of Electra can be long to train, so you can take data sample (>= 5000 and set seed for reproducibility)\n","- You may want to try freezing (do not update the pretrained model weights) all the layers exept the ones for classification, in that case use:\n","\n","\n","```\n","for param in model.electra.parameters():\n","      param.requires_grad = False\n","```\n"]},{"cell_type":"code","metadata":{"id":"8yqAAFqZcwbu","executionInfo":{"status":"ok","timestamp":1648762632640,"user_tz":-120,"elapsed":529,"user":{"displayName":"Вероника Катаева","userId":"08423665326520322322"}}},"source":["MODEL_NAME = \"google/electra-small-generator\"\n","TOKENIZER_NAME = \"google/electra-base-generator\""],"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["* load tokenizer and model"],"metadata":{"id":"aw43SFhqnPxC"}},{"cell_type":"code","source":["tokenizer = ElectraTokenizer.from_pretrained(TOKENIZER_NAME)\n","model = ElectraForMaskedLM.from_pretrained(MODEL_NAME)"],"metadata":{"id":"UIyKpRJvnM4r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model"],"metadata":{"id":"fF3HGpDa6Waw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* look at the predictions of the model as-is before any fine-tuning\n","* The easiest way to get predictions is to use transformers pipeline function"],"metadata":{"id":"cxtkxSibqY3a"}},{"cell_type":"code","source":["masked = pipeline('fill-mask', model=model, tokenizer=tokenizer)"],"metadata":{"id":"lBB850kgqm_6","executionInfo":{"status":"ok","timestamp":1648762662749,"user_tz":-120,"elapsed":2,"user":{"displayName":"Вероника Катаева","userId":"08423665326520322322"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["masked(\"Why don't you ask [MASK]?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GMNsVf3ArF9g","executionInfo":{"status":"ok","timestamp":1648762663327,"user_tz":-120,"elapsed":257,"user":{"displayName":"Вероника Катаева","userId":"08423665326520322322"}},"outputId":"24901ff5-3127-4c96-97fc-7c309800ae71"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'score': 0.5342992544174194,\n","  'sequence': \"why don't you ask me?\",\n","  'token': 2033,\n","  'token_str': 'm e'},\n"," {'score': 0.08196018636226654,\n","  'sequence': \"why don't you ask questions?\",\n","  'token': 3980,\n","  'token_str': 'q u e s t i o n s'},\n"," {'score': 0.04395333677530289,\n","  'sequence': \"why don't you ask them?\",\n","  'token': 2068,\n","  'token_str': 't h e m'},\n"," {'score': 0.04017288610339165,\n","  'sequence': \"why don't you ask why?\",\n","  'token': 2339,\n","  'token_str': 'w h y'},\n"," {'score': 0.030024440959095955,\n","  'sequence': \"why don't you ask yourself?\",\n","  'token': 4426,\n","  'token_str': 'y o u r s e l f'}]"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["masked(\"What is [MASK]\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zShxYUmqrNgC","executionInfo":{"status":"ok","timestamp":1648762666209,"user_tz":-120,"elapsed":3,"user":{"displayName":"Вероника Катаева","userId":"08423665326520322322"}},"outputId":"2ddd3e05-eeb8-4613-dfbd-e0c08203129a"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'score': 0.9262322783470154,\n","  'sequence': 'what is?',\n","  'token': 1029,\n","  'token_str': '?'},\n"," {'score': 0.05156780779361725,\n","  'sequence': 'what is.',\n","  'token': 1012,\n","  'token_str': '.'},\n"," {'score': 0.021510401740670204,\n","  'sequence': 'what is!',\n","  'token': 999,\n","  'token_str': '!'},\n"," {'score': 0.0001196492012240924,\n","  'sequence': 'what is -',\n","  'token': 1011,\n","  'token_str': '-'},\n"," {'score': 0.00010928419214906171,\n","  'sequence': 'what is \"',\n","  'token': 1000,\n","  'token_str': '\"'}]"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["masked(\"Let's talk about [MASK] physics\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D5_FvjiPrSn5","executionInfo":{"status":"ok","timestamp":1648762666463,"user_tz":-120,"elapsed":11,"user":{"displayName":"Вероника Катаева","userId":"08423665326520322322"}},"outputId":"d20387eb-a1c3-4ef7-9b85-f72a7be974ab"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'score': 0.24027501046657562,\n","  'sequence': \"let's talk about quantum physics\",\n","  'token': 8559,\n","  'token_str': 'q u a n t u m'},\n"," {'score': 0.21258601546287537,\n","  'sequence': \"let's talk about theoretical physics\",\n","  'token': 9373,\n","  'token_str': 't h e o r e t i c a l'},\n"," {'score': 0.056394025683403015,\n","  'sequence': \"let's talk about particle physics\",\n","  'token': 10811,\n","  'token_str': 'p a r t i c l e'},\n"," {'score': 0.0332079641520977,\n","  'sequence': \"let's talk about real physics\",\n","  'token': 2613,\n","  'token_str': 'r e a l'},\n"," {'score': 0.022627945989370346,\n","  'sequence': \"let's talk about mathematical physics\",\n","  'token': 8045,\n","  'token_str': 'm a t h e m a t i c a l'}]"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["* get the masked word prediction (sample sentences above) on the fine-tuned model, why the results as they are and what should be done in order to change that (write down your answer)"],"metadata":{"id":"9LhhHaQchqjI"}},{"cell_type":"code","source":["model = ElectraForMaskedLM.from_pretrained('/content/drive/MyDrive/Colab Notebooks/University/Advanced NLP/model')"],"metadata":{"id":"4vWiJN7JXwws"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model"],"metadata":{"id":"pSug9nI86l0h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["masked = pipeline('fill-mask', model=model, tokenizer=tokenizer)"],"metadata":{"id":"YsOHnLS5ijf_","executionInfo":{"status":"ok","timestamp":1648762682984,"user_tz":-120,"elapsed":17,"user":{"displayName":"Вероника Катаева","userId":"08423665326520322322"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["masked(\"Why don't you ask [MASK]?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EpTgDrEuinH3","executionInfo":{"status":"ok","timestamp":1648762682984,"user_tz":-120,"elapsed":15,"user":{"displayName":"Вероника Катаева","userId":"08423665326520322322"}},"outputId":"66e6ca8f-b30b-462a-bcf9-2b77514761f7"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'score': 0.007874231785535812,\n","  'sequence': \"why don't you ask?\",\n","  'token': 0,\n","  'token_str': '[ P A D ]'},\n"," {'score': 0.006127915345132351,\n","  'sequence': \"why don't you ask horn?\",\n","  'token': 7109,\n","  'token_str': 'h o r n'},\n"," {'score': 0.005303030833601952,\n","  'sequence': \"why don't you ask flap?\",\n","  'token': 20916,\n","  'token_str': 'f l a p'},\n"," {'score': 0.004756000358611345,\n","  'sequence': \"why don't you ask felipe?\",\n","  'token': 17095,\n","  'token_str': 'f e l i p e'},\n"," {'score': 0.004418663680553436,\n","  'sequence': \"why don't you askrix?\",\n","  'token': 17682,\n","  'token_str': '# # r i x'}]"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["masked(\"What is [MASK]\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6FSlumqSio72","executionInfo":{"status":"ok","timestamp":1648762683324,"user_tz":-120,"elapsed":353,"user":{"displayName":"Вероника Катаева","userId":"08423665326520322322"}},"outputId":"6aae50c3-3c3e-4bfc-e496-d205771dab22"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'score': 0.005034726113080978,\n","  'sequence': 'what israße',\n","  'token': 27807,\n","  'token_str': '# # r a ß e'},\n"," {'score': 0.004568912088871002,\n","  'sequence': 'what is schwarz',\n","  'token': 29058,\n","  'token_str': 's c h w a r z'},\n"," {'score': 0.004546832758933306,\n","  'sequence': 'what is headline',\n","  'token': 17653,\n","  'token_str': 'h e a d l i n e'},\n"," {'score': 0.004321066662669182,\n","  'sequence': 'what is worrying',\n","  'token': 15366,\n","  'token_str': 'w o r r y i n g'},\n"," {'score': 0.004065139684826136,\n","  'sequence': 'what isriding',\n","  'token': 21930,\n","  'token_str': '# # r i d i n g'}]"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["masked(\"Let's talk about [MASK] physics\")"],"metadata":{"id":"U5q98KVOis3q","executionInfo":{"status":"ok","timestamp":1648762683325,"user_tz":-120,"elapsed":16,"user":{"displayName":"Вероника Катаева","userId":"08423665326520322322"}},"outputId":"56ac7d16-e970-43c5-bd3f-d7ab7a939bf7","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'score': 0.009592956863343716,\n","  'sequence': \"let's talk aboutasa physics\",\n","  'token': 16782,\n","  'token_str': '# # a s a'},\n"," {'score': 0.008506628684699535,\n","  'sequence': \"let's talk about lateral physics\",\n","  'token': 11457,\n","  'token_str': 'l a t e r a l'},\n"," {'score': 0.00676969438791275,\n","  'sequence': \"let's talk about leyte physics\",\n","  'token': 27214,\n","  'token_str': 'l e y t e'},\n"," {'score': 0.0054626683704555035,\n","  'sequence': \"let's talk aboutriding physics\",\n","  'token': 21930,\n","  'token_str': '# # r i d i n g'},\n"," {'score': 0.0044946749694645405,\n","  'sequence': \"let's talk about statewide physics\",\n","  'token': 13486,\n","  'token_str': 's t a t e w i d e'}]"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["**Conclusion:** ELECTRA was pre-trained on a large dataset, which consists of 3.3 Billion tokens from Wikipedia and BooksCorpus. The pre-training objective of the training was related to Masked Language Modeling. Therefore, the model performs more confident (higher probabilities) and more understandable to human results.  \n","ELECTRA architecture was trained by me with all trainable layers that hold pre-trained features on a classification task. The produced results follow the peculiarities of the dataset content."],"metadata":{"id":"KO3KEDmBAn97"}},{"cell_type":"code","source":[""],"metadata":{"id":"TlPDFodJiwtQ"},"execution_count":null,"outputs":[]}]}